{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Deep Learning\n",
    "from keras.layers import Conv2D, Dense, Dropout, Flatten\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Ensure consistency across runs\n",
    "from numpy.random import seed\n",
    "import random\n",
    "seed(2)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "# Imports to view data\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "from keras.utils import print_summary\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Utils\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import getenv\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# Image Preprocessing\n",
    "from skimage.filters import sobel, scharr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables\n",
    "TRAIN_DIR = '../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\n",
    "TEST_DIR = '../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test'\n",
    "CUSTOM_TEST_DIR = '../input/asl-alphabet-test/asl-alphabet-test'\n",
    "CLASSES = [folder[len(TRAIN_DIR) + 1:] for folder in glob(TRAIN_DIR + '/*')]\n",
    "CLASSES.sort()\n",
    "\n",
    "TARGET_SIZE = (64, 64)\n",
    "TARGET_DIMS = (64, 64, 3) # add channel for RGB\n",
    "N_CLASSES = 29\n",
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Model saving for easier local iterations\n",
    "MODEL_DIR = '../input/aslalphabetcnnmodel1'\n",
    "MODEL_PATH = MODEL_DIR + '/cnn-model.h5'\n",
    "MODEL_WEIGHTS_PATH = MODEL_DIR + '/cnn-model.weights.h5'\n",
    "MODEL_SAVE_TO_DISK = getenv('KAGGLE_WORKING_DIR') != '/kaggle/working'\n",
    "\n",
    "print('Save model to disk? {}'.format('Yes' if MODEL_SAVE_TO_DISK else 'No'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_sample_of_each(base_path):\n",
    "    cols = 5\n",
    "    rows = int(np.ceil(len(CLASSES) / cols))\n",
    "    fig = plt.figure(figsize=(16, 20))\n",
    "    \n",
    "    for i in range(len(CLASSES)):\n",
    "        cls = CLASSES[i]\n",
    "        img_path = base_path + '/' + cls + '/**'\n",
    "        path_contents = glob(img_path)\n",
    "    \n",
    "        imgs = random.sample(path_contents, 1)\n",
    "\n",
    "        sp = plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(cv2.imread(imgs[0]))\n",
    "        plt.title(cls)\n",
    "        sp.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_sample_of_each(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_sample_of_each(CUSTOM_TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    '''Function that will be implied on each input. The function\n",
    "    will run after the image is resized and augmented.\n",
    "    The function should take one argument: one image (Numpy tensor\n",
    "    with rank 3), and should output a Numpy tensor with the same\n",
    "    shape.'''\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    return sobely\n",
    "\n",
    "def make_generator(options):\n",
    "    '''Creates two generators for dividing and preprocessing data.'''\n",
    "    validation_split = options.get('validation_split', 0.0)\n",
    "    preprocessor = options.get('preprocessor', None)\n",
    "    data_dir = options.get('data_dir', TRAIN_DIR)\n",
    "\n",
    "    augmentor_options = {\n",
    "        'samplewise_center': True,\n",
    "        'samplewise_std_normalization': True,\n",
    "    }\n",
    "    if validation_split is not None:\n",
    "        augmentor_options['validation_split'] = validation_split\n",
    "    \n",
    "    if preprocessor is not None:\n",
    "        augmentor_options['preprocessing_function'] = preprocessor\n",
    "    \n",
    "    flow_options = {\n",
    "        'target_size': TARGET_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'shuffle': options.get('shuffle', None),\n",
    "        'subset': options.get('subset', None),\n",
    "    }\n",
    "\n",
    "    data_augmentor = ImageDataGenerator(**augmentor_options)\n",
    "    return data_augmentor.flow_from_directory(data_dir, **flow_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_disk():\n",
    "    '''A convenience method for re-running certain parts of the\n",
    "    analysis locally without refitting all the data.'''\n",
    "    model_file = Path(MODEL_PATH)\n",
    "    model_weights_file = Path(MODEL_WEIGHTS_PATH)\n",
    "                      \n",
    "    if model_file.is_file() and model_weights_file.is_file():\n",
    "        print('Retrieving model from disk...')\n",
    "        model = load_model(model_file.__str__())\n",
    "                      \n",
    "        print('Loading CNN model weights from disk...')\n",
    "        model.load_weights(model_weights_file)\n",
    "        return model\n",
    "    \n",
    "    return None\n",
    "\n",
    "CNN_MODEL = load_model_from_disk()\n",
    "REPROCESS_MODEL = (CNN_MODEL is None)\n",
    "\n",
    "print('Need to reprocess? {}'.format(REPROCESS_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(save=False):\n",
    "    print('Building model afresh...')\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=TARGET_DIMS))\n",
    "    model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n",
    "    model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n",
    "    model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(N_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    if save: model.save(MODEL_PATH)\n",
    "        \n",
    "    return model\n",
    "\n",
    "if REPROCESS_MODEL:\n",
    "    CNN_MODEL = build_model(save=MODEL_SAVE_TO_DISK)\n",
    "\n",
    "print_summary(CNN_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_for(subset):\n",
    "    '''Create a generator for the training or validation set.'''\n",
    "    generator_options = dict(\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        shuffle=True,\n",
    "        subset=subset,\n",
    "        preprocessor=preprocess_image,\n",
    "    )\n",
    "    return make_generator(generator_options)\n",
    "\n",
    "\n",
    "def fit_model(model, train_generator, val_generator, save=False):\n",
    "    '''Fit the model with the training and validation generators.'''    \n",
    "    history = model.fit_generator(train_generator, epochs=5, validation_data=val_generator)\n",
    "    \n",
    "    if save: model.save_weights(MODEL_WEIGHTS_PATH)\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "CNN_TRAIN_GENERATOR = make_generator_for('training')\n",
    "CNN_VAL_GENERATOR = make_generator_for('validation')\n",
    "\n",
    "HISTORY = None\n",
    "if REPROCESS_MODEL:\n",
    "    start_time = time.time()\n",
    "    HISTORY = fit_model(CNN_MODEL, CNN_TRAIN_GENERATOR, CNN_VAL_GENERATOR, save=MODEL_SAVE_TO_DISK)\n",
    "    print('Fitting the model took ~{:.0f} second(s).'.format(time.time() - start_time))\n",
    "\n",
    "\n",
    "columns=['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4']\n",
    "pd.DataFrame(data=[x.shape for x in CNN_MODEL.weights], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HISTORY:\n",
    "    print('Final Accuracy: {:.2f}%'.format(HISTORY.history['acc'][4] * 100))\n",
    "    print('Validation set accuracy: {:.2f}%'.format(HISTORY.history['val_acc'][4] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
